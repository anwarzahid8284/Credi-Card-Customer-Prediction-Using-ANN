# ğŸ§  Credit Card Customer Churn Using ANN (Artificial Neural Network)

This project demonstrates how to build and train an **Artificial Neural Network (ANN)** from scratch using **TensorFlow/Keras** to predict whether a **credit card customer will churn (leave the bank)** based on their profile data.

---

## ğŸ¯ Main Learning Purpose

> The **primary goal** of this project is to **understand how an ANN works**, not just to achieve high accuracy.
>
> You'll learn how data moves through:
> - **Forward propagation**
> - **Activation functions**
> - **Weight updates via backpropagation**
> - And how to improve or ruin model performance with architecture and training tweaks

---

## ğŸ’¡ What This Project Does

This project walks you through the **complete machine learning pipeline**:

1. ğŸ“¥ **Loads a dataset** of 10,000 bank customers with features like age, credit score, salary, etc.
2. ğŸ§¹ **Preprocesses the data** by:
   - Dropping unnecessary columns
   - Encoding categorical data
   - Scaling numerical features
3. ğŸ§  **Builds an ANN** with:
   - 1 hidden layer (3â€“6 neurons, ReLU)
   - 1 output layer (Sigmoid for binary classification)
4. ğŸ§ª **Trains the ANN** using:
   - `binary_crossentropy` loss
   - `adam` optimizer
   - 10 to 100 epochs
5. ğŸ“Š **Displays accuracy and loss graphs** for both training and validation sets
6. ğŸ§ª **Experiments** with architecture changes to observe how they affect learning
7. âš ï¸ **Discusses overfitting** and what causes it

---

## ğŸ“Š Project Overview

- ğŸ§  **Type**: Binary Classification  
- ğŸ“ **Data**: 10,000 records with 14 features  
- ğŸ¯ **Target**: `Exited` column (1 = churn, 0 = not churn)  
- âš™ï¸ **Frameworks**: TensorFlow + Keras  
- ğŸ“ **Focus**: Educational â€” learn internals of ANN

---

## ğŸ§¼ Preprocessing Steps

1. ğŸ—‘ï¸ **Drop Columns**: `RowNumber`, `CustomerId`, `Surname`
2. ğŸ§  **One-Hot Encoding**: `Geography`, `Gender`
3. âš–ï¸ **Feature Scaling**: Normalized numerical columns using `StandardScaler`

---

## ğŸ¯ Improving Model Accuracy

| Change                    | Why it helps                                   |
|---------------------------|------------------------------------------------|
| ğŸ”§ ReLU Activation         | Avoids vanishing gradients                     |
| â• More Neurons / Layers   | Allows model to learn more complex patterns    |
| ğŸ” More Epochs (up to 100) | Helps model train longer to improve accuracy   |

---

## âš ï¸ Overfitting Discussion

| Topic         | Explanation                                                                 |
|---------------|-----------------------------------------------------------------------------|
| â— What is it? | When a model memorizes training data but fails to generalize                |
| ğŸ” Symptoms    | Training accuracy â†‘, validation accuracy â†“ or stagnant                     |
| ğŸ§ª In This Project | Appeared with more epochs/layers without regularization                 |
| ğŸ§° Fix (Future) | Add Dropout, EarlyStopping, reduce complexity                             |

---

## ğŸ“‰ Model Loss Plot

Visual representation of **training vs validation loss**:

![Loss Curve](loss.png)

---

## ğŸ“ˆ Model Accuracy Plot

Visual representation of **training vs validation accuracy**:

![Accuracy Curve](accuracy.png)

---

## ğŸ“¬ Summary

By the end of this project, youâ€™ll understand:

- ğŸ” How forward and backward propagation work in ANN
- âš™ï¸ How tuning neurons, layers, and epochs affects learning
- ğŸ“ˆ How to monitor performance using loss/accuracy graphs
- âš ï¸ How overfitting happens and how to detect it

---

> ğŸ’¬ **Learning is my hobby**  
> ğŸ˜„ â€œOthers play games to unwind â€” I play with neural networks. Watching loss go down is my version of Netflix!â€

---

## ğŸ‘¨â€ğŸ’» Author

**Anwar Zahid**  
ğŸ“§ your.email@example.com  
ğŸŒ GitHub: [YourGitHubUsername](https://github.com/YourGitHubUsername)

---
